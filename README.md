# üïµüèª Detecting Training Data of Large Language Models via Expectation Maximization

[![arXiv](https://img.shields.io/badge/arXiv-2410.xxxxx-b31b1b.svg)](https://arxiv.org/abs/2410.xxxxx)

This is the repository for the official implementation of [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.xxxxx).

## Acknowledgement
This codebase is adapted from the following repositories: [Min-K%](https://github.com/swj0419/detect-pretrain-code), [Min-K%++](https://github.com/zjysteven/mink-plus-plus), and [ReCaLL](https://github.com/ruoyuxie/recall).


## Citation
‚≠ê If you find our work (paper, implementation, and datasets) helpful, please consider citing our paper:
```bibtex
@article{kim2024detecting,
  title={Detecting Training Data of Large Language Models via Expectation Maximization},
  author={Kim, Gyuwan and Li, Yang and Spiliopoulou, Evangelia and Ma, Jie and Ballesteros, Miguel and Wang, William Yang},
  journal={arXiv preprint arXiv:2410.xxxxx},
  year={2024}
}
```


## Contact
For any inqury, please open an [issue](https://github.com/gyuwankim/em-mia/issues) or [contact](https://gyuwankim.github.io) the authors directly.
